# Priority 4: User Testing & Validation Framework
## Scribbly Educational App Research

**Date:** December 4, 2025  
**Context:** Evidence-based user testing protocols for validating Scribbly's trace-to-create learning model with 6-year-olds and their parents

---

## Executive Summary

User testing with children requires specialized protocols that balance research rigor with ethical protection of vulnerable participants. This framework provides Scribbly with evidence-based methodologies for:

1. **Ethical child testing** that prioritizes safety and meaningful consent
2. **Age-appropriate UX methods** tailored to 6-year-old capabilities
3. **Parent interview frameworks** to validate educational value
4. **Prototyping strategies** that maximize learning before heavy investment
5. **Learning outcome metrics** that measure both skill development and engagement
6. **Sample size guidance** to balance quality insights with resource efficiency

**Critical Finding:** Testing with 8-10 child-parent pairs across 2-3 iterative rounds will reveal 80-90% of usability issues while establishing preliminary evidence of learning effectiveness. Combining qualitative observation of children with semi-structured parent interviews provides the dual validation needed for both user experience and educational impact.

---

## 1. Ethical Framework for Testing with Children

### 1.1 Core Ethical Principles (Federal & Research Standards)

Testing with children aged 6 requires adherence to four bioethical principles:

**Beneficence (Do Good)**
- Research must provide potential benefit to child participants
- For Scribbly: Testing validates that app actually helps kids learn letters and build creative confidence
- Educational apps fall under "minimal risk" research when properly designed

**Non-Maleficence (Do No Harm)**
- Avoid physical, psychological, or emotional harm
- For Scribbly: No performance anxiety, no failure states, no comparative scoring
- Monitor for frustration, boredom, or fatigue during sessions

**Autonomy (Respect Choice)**
- Children at age 6 can provide "assent" (not legal consent, which parents give)
- Child must willingly agree to participate; can stop anytime
- For Scribbly: Child chooses whether to test; can take breaks; controls exit

**Justice (Fair Treatment)**
- Include diverse participants (SES, race, ability, language)
- Don't exploit vulnerable populations
- For Scribbly: Test with range of fine motor abilities, handwriting experience, cultural backgrounds

### 1.2 Consent & Assent Requirements

**Parental Consent (Legal Requirement)**
- Written consent from parent/guardian before any testing
- Must explain: purpose, procedures, time commitment, risks, benefits, right to withdraw
- Plain language (8th grade reading level or lower)
- Available in multiple languages if testing diverse populations

**Child Assent (Ethical Requirement)**
- Age-appropriate explanation: "We're making a new app to help kids learn letters and draw cool things. We want to see if it's fun and easy to use. You can try it and tell us what you think!"
- Child must verbally agree
- Explain they can stop anytime, no consequences
- Watch for non-verbal cues of discomfort (even if child says "yes" to please adults)

**Sample Child Assent Script:**
> "Hi! We made a drawing app where you trace letters and then turn them into art. Want to try it and tell us if you like it? You can stop whenever you want, and there are no wrong answers. Does that sound fun?"

### 1.3 Protecting Children During Testing

**Environmental Safeguards:**
- Test in familiar, comfortable settings (home, classroom, or child-friendly lab)
- Parent/caregiver present or nearby (but not hovering over shoulder)
- Private space free from distractions
- Age-appropriate furniture (no wheels on chairs for younger kids)

**Session Design Safeguards:**
- Maximum 15 minutes of active testing (10-12 optimal for age 6)
- Built-in breaks
- Morning or early afternoon sessions (avoid late day when tired)
- Never test when child is sick, hungry, or upset

**Psychological Safeguards:**
- No performance pressure ("This isn't a test of YOU, we're testing the app!")
- Positive language only ("That's an interesting way to do it!")
- Never label actions as "mistakes" or "wrong"
- Acknowledge effort, not just outcomes
- If child becomes frustrated, offer break or end session

**Data Privacy (COPPA Compliance):**
- No collection of personal info beyond what's necessary
- No recording of faces without explicit consent
- Secure storage of any video/audio recordings
- Anonymize all data in reports
- Delete recordings after analysis (or store securely per consent)

### 1.4 Red Flags to Stop Testing Immediately

- Child explicitly asks to stop
- Signs of distress (crying, shutting down, aggression)
- Physical discomfort (bathroom needs, hunger, pain)
- Sustained disengagement (child stops responding, stares blankly)
- Parent intervenes to stop

**Remember:** It's better to lose a testing session than to harm a child's trust or well-being.

---

## 2. UX Testing Methodologies for 6-Year-Olds

### 2.1 How Children Age 6 Differ from Adults

**Cognitive Differences:**
- Explore through trial-and-error, not logic
- Short attention spans (10-15 minutes educational tasks)
- Still learning to read (can't rely on text)
- Concrete thinkers (abstractions difficult)
- Limited ability to articulate problems verbally

**Motor Differences:**
- Developing fine motor control (still learning pencil grip)
- Less precision with small touch targets
- May tire quickly from sustained tracing
- Wide variation in abilities at same age

**Social/Emotional Differences:**
- Want to please adults (may hide frustration)
- Fear of failure emerges at age 6 (judgment-sensitive)
- May not voice confusion to avoid "being wrong"
- React more to immediate feedback than delayed consequences

**Research Implication:** Rely on observation of behavior MORE than verbal feedback. What they do reveals more than what they say.

### 2.2 Testing Formats for Age 6

**Option A: Individual Sessions (Recommended for Scribbly MVP)**
- One child, one facilitator, one parent nearby
- Best for observing natural interaction without peer influence
- Child more likely to voice confusion without social pressure
- Easier to control session pace and stopping points

**Pros:** Pure individual behavior data; flexible pacing; easier to schedule
**Cons:** More sessions needed; no peer learning observations; can feel "formal"

**Option B: Paired Sessions (Ages 6-8)**
- Two children who know each other (friends, siblings)
- Take turns using app while other watches
- Nielsen Norman Group found this works well for 6-8 age group
- Kids feel more comfortable with familiar peer present

**Pros:** More natural behavior; kids explain to each other (reveals understanding); efficient
**Cons:** One may dominate; harder to control; scheduling complexity

**Recommendation for Scribbly MVP Testing:** Start with **individual sessions** to get clean baseline data on usability. Consider paired sessions in later rounds to observe social learning and sharing of creative work.

### 2.3 Session Structure (12-Minute Format)

**Phase 1: Warm-Up (2 minutes)**
- Introduce yourself in friendly way
- Show them device/tablet
- "This isn't a test of you, I want to see if OUR app works well"
- Ask about their experience with tablets/drawing

**Phase 2: Task Observation (7-8 minutes)**
- Give minimal instructions: "Can you show me what you would do here?"
- Let them explore naturally
- Observe:
  - Where do they look first?
  - What do they tap?
  - Do they understand the trace/creative flow?
  - Signs of confusion (tapping repeatedly, looking at parent)
  - Signs of delight (smiles, "look at this!")
  - When do they need help?

**Phase 3: Gentle Inquiry (3 minutes)**
- Ask simple, open questions:
  - "What did you think of that?"
  - "What was your favorite part?"
  - "Was anything hard or confusing?"
  - "Would you want to use this again?"
- Use "magic wand" technique: "If you could change anything with a magic wand, what would you change?"

**AVOID:**
- "Did you like it?" (leading, they'll say yes)
- "Why did you do that?" (sounds accusatory)
- "Was that easy?" (leading)
- Interrupting their flow to ask questions

### 2.4 Facilitator Best Practices

**Do:**
- Smile and use encouraging tone
- Normalize "testing the app, not the child"
- Stay quiet during tasks (resist urge to help)
- Take notes on behaviors, not judgments
- Follow the child's lead
- Celebrate their creativity ("I love how you turned that into a dinosaur!")

**Don't:**
- Hover or crowd the child
- Correct their approach
- Show disappointment when they struggle
- Ask leading questions
- Over-explain (give minimal instructions)
- Pressure them to continue if disengaged

**Managing Parents:**
- Brief them beforehand: "We want to see how YOUR child naturally uses it, so try not to help unless they ask"
- Position parent to side, not directly behind child
- If parent jumps in, gently redirect: "It's okay if they struggle a bit, we learn a lot from that!"

### 2.5 Non-Verbal Observation Cues (More Important Than Words!)

**Positive Engagement:**
- Leaning forward toward screen
- Smiling, laughing
- Showing parent: "Look at this!"
- Repeated use of features
- Finishing tasks without prompting

**Confusion:**
- Tapping same area repeatedly
- Looking away from screen to facilitator/parent
- Pausing mid-task for extended time
- Random tapping (not purposeful)
- Asking "What do I do?"

**Frustration:**
- Furrowed brow, frowning
- Aggressive tapping
- Pushing device away
- Slumping posture
- "I don't like this" or "This is hard"

**Fatigue:**
- Yawning, rubbing eyes
- Decreased engagement mid-session
- Slower responses
- Looking around room instead of screen

**Boredom:**
- Monotone responses
- Going through motions without enthusiasm
- "Can I be done now?"
- Looking at other objects in room

### 2.6 Incentives for Children

**Effective Incentives:**
- Small toy/sticker at end (builds excitement)
- Choice from "prize bin" 
- Certificate of "App Testing Expert"
- Get to keep their artwork printed out
- Gift card for parents (usually $25-50 for 30 min including parent interview)

**Poor Incentives:**
- Money directly to child (parent may control anyway)
- Large prizes (creates pressure to perform)
- Competitive elements (contradicts the learning)

---

## 3. Parent/Caregiver Interview Framework

Parents are your validation that Scribbly provides *real educational value* beyond just being "fun." Semi-structured interviews (10-15 minutes after child testing) provide critical insights.

### 3.1 Interview Goals

1. **Validate educational value perception**
2. **Understand purchase decision factors**
3. **Identify home learning context**
4. **Surface concerns or barriers**
5. **Gather comparative data** (what else they use/tried)

### 3.2 Parent Interview Questions (Core Set)

**Educational Value & Learning Goals**
1. "What are you hoping [Child's name] learns or gets better at with handwriting and letters right now?"
2. "After watching [Child] use the app, do you think it would help with those goals? Why or why not?"
3. "How important is it to you that learning apps combine education with creativity?"
4. "What would make you confident that an app is actually teaching something, not just entertaining?"

**Current Solutions & Pain Points**
5. "What other apps, workbooks, or activities do you currently use to help [Child] with letters and writing?"
6. "What works well with those? What doesn't work or frustrates you?"
7. "How much time does [Child] currently spend on educational apps per day/week?"
8. "Do you find it hard to find apps that are educational AND that [Child] wants to use?"

**Product Reaction & Fit**
9. "What was your first impression watching [Child] use Scribbly?"
10. "Did anything surprise youâ€”good or badâ€”about how [Child] interacted with it?"
11. "On a scale of 1-10, how likely would you be to download this app for [Child]? What would need to change to make it a 10?"
12. "Would you use this with [Child], or let them use it independently?"

**Purchase Intent & Barriers**
13. "If this app were available today, what would you want to know before downloading it?"
14. "What price point feels fair for an app like this?" (Show pricing tiers: Free with limits, $7.99/month, $49.99/year)
15. "What would be a deal-breakerâ€”something that would make you definitely NOT use this app?"

**Context & Demographics** (If not already collected)
16. "How would you describe [Child's] handwriting level right now?" (Just starting, learning letters, can write name, writes sentences)
17. "Does [Child] have experience with tablets or drawing apps?"
18. "Are there any special considerations we should know about?" (Fine motor challenges, language learning, etc.)

### 3.3 Interview Best Practices

**Do:**
- Build rapport first (compliment their child's creativity)
- Listen actively; probe deeper on interesting comments
- Acknowledge their expertise ("You know your child best")
- Ask "Why?" and "Tell me more about that"
- Take detailed notes (or record with permission)

**Don't:**
- Defend app design if they criticize (you want honest feedback!)
- Lead them to positive answers
- Use jargon or technical terms
- Rush through questions

### 3.4 Additional Stakeholder Interviews (Optional Later Phases)

**Educators/Teachers:**
- Interview K-1 teachers about classroom needs
- Questions about curriculum alignment, classroom feasibility, progress tracking needs
- Ideal sample: 3-5 teachers in different school settings

**Occupational Therapists:**
- Validate haptic feedback approach
- Get input on motor development considerations
- Ideal sample: 2-3 OTs specializing in pediatric handwriting

---

## 4. Prototyping Strategy

### 4.1 Fidelity Levels Explained

**Low-Fidelity (Lo-Fi):**
- Paper sketches, clickable wireframes, basic layouts
- Fast to create, easy to change, cheap
- Focus on flow and structure, NOT visuals
- Best for: Early concept testing, exploring alternatives, internal team alignment

**High-Fidelity (Hi-Fi):**
- Realistic visuals, colors, animations, branding
- Feels like real app, more time/cost to create
- Focus on realistic interactions and polish
- Best for: Usability testing, investor demos, final validation before build

### 4.2 Recommended Prototyping Path for Scribbly

**PHASE 1: Paper/Lo-Fi Prototypes (Week 1-2)**
- Sketch 3 different approaches to trace-to-create flow
- Test internally with team + 2-3 parent colleagues (not formal testing)
- Goal: Validate core concept, identify best flow
- Tools: Paper, Figma wireframes, InVision basic clicks

**Key Questions:**
- Is traceâ†’creative transition clear?
- Where do kids expect haptic feedback slider?
- Is "fade away" understandable?
- Do parents get the educational model?

**PHASE 2: Medium-Fidelity Interactive Prototype (Week 3-5)**
- Build clickable prototype with 2-3 letters (e.g., A, S, O - different shapes)
- Simple visual style, basic interactions
- Fake haptic feedback (can't prototype vibration, but show sliders)
- Test with 5-7 child-parent pairs

**Key Questions:**
- Can 6-year-olds navigate the UI independently?
- Does tracing feel natural with finger/stylus?
- Do kids understand they CAN'T fail?
- Where do they get stuck or confused?
- Do they spontaneously move from trace to creative?

**Tools:** Figma with prototyping, Adobe XD, or ProtoPie

**PHASE 3: High-Fidelity Prototype (Week 6-8)**
- Polish visuals: colors, animations, character fade effect, creative tools
- 5-7 full letters with complete traceâ†’creative cycles
- Realistic artwork examples
- Test with 8-10 child-parent pairs (can recruit new or re-test previous)

**Key Questions:**
- Does it feel "real" enough to validate learning?
- Are kids delighted? Do they want to keep going?
- Would parents actually pay for this?
- Any last usability issues before development?

### 4.3 What You Can't Prototype (Test Early in Real Code)

Some Scribbly features require actual development to test properly:

**Haptic Feedback:**
- Cannot simulate vibration in Figma/InVision
- Build simple demo app with real haptics EARLY (even if ugly visuals)
- Test: Can kids feel difference between off/low/high? Does it help or distract?

**Drawing/Tracing Precision:**
- Prototyping tools don't capture real touch tracking
- Need actual drawing engine to test "Â¼ inch tolerance" rules
- Test: Is tolerance too strict (frustrating) or too loose (not challenging)?

**Performance:**
- Lag, loading times, responsiveness must be tested in real app
- Prototypes don't reveal technical performance issues

**Recommendation:** After Phase 2 prototype testing, build a "technical demo" app with 1-2 letters that tests haptics, drawing engine, and fade animationsâ€”even if UI is rough. Test this with 3-5 kids to validate critical technical features before full build.

---

## 5. Sample Size Recommendations

### 5.1 The "5-User Rule" and When It Applies

**Nielsen Norman Group's Famous Rule:** Testing with 5 users reveals ~85% of usability issues.

**When it works:**
- Qualitative usability testing (finding problems, not measuring performance)
- Homogeneous user group (similar ages, abilities, behaviors)
- Moderate-complexity interfaces
- Iterative testing (testâ†’fixâ†’test again)

**When it DOESN'T work:**
- Quantitative metrics (completion rates, time on task, statistical comparisons)
- Diverse user groups (kids with wide ability range)
- High-stakes products (medical, safety-critical)
- One-shot testing (no iteration planned)

### 5.2 Recommended Sample Sizes for Scribbly Testing

**Prototype Testing (Qualitative - Finding Usability Issues)**

| Phase | Participants | Rationale |
|-------|--------------|-----------|
| Lo-Fi Paper Prototype | 3-4 parent colleagues | Internal validation, fast feedback |
| Med-Fi Interactive Prototype | 5-7 child-parent pairs | Uncover 80-85% of issues, diverse abilities |
| Hi-Fi Polished Prototype | 8-10 child-parent pairs | Catch remaining 10-15%, validate improvements |
| **Total Unique Participants** | **13-17 child-parent pairs** | Accounts for overlap if re-testing some |

**Why 8-10 for final round instead of 5?**
- Children age 6 have more behavioral variability than adults
- Fine motor skills vary widely (some kids write sentences, others still learning letters)
- Testing reveals issues that happen at p=0.10-0.15 probability (rarer problems)
- Better statistical confidence for parent purchase intent data

**Learning Outcomes Testing (Quantitative - Measuring Improvement)**

*Not needed for MVP validation, but critical for post-launch research and marketing claims*

| Goal | Sample Size | Study Design |
|------|-------------|--------------|
| Show "kids improved" | 20-30 kids | Pre-test writing, use app 4 weeks, post-test writing |
| Compare to control | 40-60 kids | 20-30 Scribbly group, 20-30 control (workbooks or other app) |
| Publish peer-reviewed paper | 60-100+ kids | Needed for statistical power in educational research |

**Recommendation:** For MVP launch, focus on qualitative usability testing (13-17 pairs). After launch, run a longitudinal study with 30-40 kids to measure learning gains for marketing and credibility.

### 5.3 Participant Diversity Requirements

For a sample of 8-10 child-parent pairs, aim for diversity across:

**Fine Motor Ability:**
- 2-3 kids who are "ahead" (already writing well)
- 4-5 kids at "grade level" (learning letters, some reversals)
- 2-3 kids who are "behind" (struggling with pencil grip, letter formation)

**Device Experience:**
- 3-4 "high use" (tablets daily)
- 3-4 "medium use" (tablets 2-3x/week)
- 1-2 "low use" (minimal tablet experience)

**Demographics:**
- Mix of boys and girls (6-year-olds don't show major gender differences in handwriting yet, but aim for 40/60 split either way)
- Racial/ethnic diversity reflecting U.S. population or target market
- Range of socioeconomic backgrounds (if possibleâ€”recruit from both public and private schools)

**Language:**
- If planning multi-language release, test with bilingual families
- Note: Cultural attitudes toward "mistake-free" learning vary; monitor for concerns

### 5.4 Practical Recruitment Tips

**Where to find participants:**
- Parent groups on Facebook, Nextdoor
- Elementary schools (coordinate with principals)
- Pediatric OT/PT clinics (kids getting handwriting support)
- Community centers, libraries
- User testing recruitment platforms (UserTesting.com, Respondent.io)

**Screening questions:**
- Age of child (must be 6, not 5 or 7)
- Can child hold a pencil/crayon and trace shapes? (basic motor requirement)
- Does child have access to tablet at home? (ensure they understand touch interface)
- Is child currently learning letters in school? (yes for target audience)

**Compensation:**
- $50-75 Amazon gift card for 30-minute session (15 min child testing + 15 min parent interview)
- Or $75-100 cash if in-person
- Sticker or small toy for child (~$5 value)

---

## 6. Learning Outcome Metrics

How do we measure if Scribbly actually *teaches* letter formation and builds creative confidence? This section provides research-backed metrics aligned with educational standards.

### 6.1 The Four Pillars of Learning Framework

Research shows effective educational apps score well on these evidence-based criteria:

**Pillar 1: Active Learning**
- Child directly manipulates content (not just watching)
- Multiple ways to achieve goals (not single path)
- Child makes meaningful choices

*Scribbly Implementation:* Tracing requires active touch input; creative phase is open-ended; child controls haptic level and creative tools.

**Pillar 2: Engagement in the Learning Process**
- Minimal distractions (no ads, limited animations)
- Feedback supports learning (not just entertainment)
- Challenges appropriately difficult (not frustrating, not boring)

*Scribbly Implementation:* No ads; judgment-free feedback; adaptive tolerance for tracing; no failure states.

**Pillar 3: Meaningful Learning**
- Content relevant to real-world skills
- Connects to existing knowledge
- Promotes deep processing (not rote repetition)

*Scribbly Implementation:* Letter tracing directly teaches handwriting; creative transformation adds personal meaning; connects motor learning to artistic expression.

**Pillar 4: Social Interaction**
- Option to share or collaborate (not required)
- Promotes discussion with adults/peers
- Builds community around learning

*Scribbly Implementation:* Save/share artwork; parent dashboard to discuss progress; potential for "gallery" to see peers' work (Phase 2).

**Assessment Tool:** Rate Scribbly 0-3 on each pillar (0=absent, 1=weak, 2=present, 3=strong). High-quality apps score 8+ total. Scribbly's target: 9-10/12.

### 6.2 Metrics for Usability Testing

**During Testing Sessions (Observational):**

| Metric | How to Measure | Target for Scribbly |
|--------|----------------|---------------------|
| Task Completion Rate | % of kids who complete traceâ†’creative cycle for 1 letter | â‰¥80% without help |
| Time on Task | Average time to trace one letter | 3-5 minutes (not rushing, not stuck) |
| Error Recovery | When kid goes off-path, can they get back? | â‰¥70% self-recover |
| Help Requests | # of times child asks parent/facilitator for help | â‰¤2 per letter |
| Engagement Duration | How long before child stops naturally | â‰¥10 minutes (2-3 letters) |
| Delight Moments | # of smiles, laughs, "look at this!" moments | â‰¥2 per session |
| Frustration Incidents | # of visible frustration signals | â‰¤1 per session |

**Post-Session (Child Feedback):**
- "Would you want to use this again?" (Yes/Maybe/No)
- "What was your favorite part?" (Trace, Creative, or Both)
- "Was anything hard or confusing?" (Open-ended)

**Target:** â‰¥80% say "Yes" to using again; â‰¥60% mention creative phase as favorite.

**Post-Session (Parent Feedback):**

| Metric | How to Measure | Target |
|--------|----------------|--------|
| Perceived Educational Value | "Does this help with handwriting?" (1-10 scale) | â‰¥7 average |
| Purchase Intent | "Would you pay for this?" (Yes/Maybe/No) | â‰¥60% Yes + Maybe |
| Price Sensitivity | "What price feels fair?" (Open-ended) | Median â‰¤$8/month or $50/year |
| Comparative Preference | "Better than [current app]?" (Yes/Same/No) | â‰¥50% Yes |

### 6.3 Metrics for Learning Effectiveness (Post-Launch Research)

**Constrained Skills (Easier to Measure, Faster to Show Impact):**

| Skill | Pre-Test | Intervention | Post-Test | Expected Improvement |
|-------|----------|--------------|-----------|----------------------|
| Letter Recognition | Show 26 letters, ask child to name | 4 weeks, 3x/week Scribbly use | Re-test letter naming | +10-15% accuracy |
| Letter Formation | Child writes 5 target letters, score legibility 1-5 | 4 weeks, 3x/week Scribbly use | Re-score same 5 letters | +0.5-1.0 points/letter |
| Correct Letter Orientation | Count # of reversals (b/d, p/q) in writing sample | 4 weeks, 3x/week Scribbly use | Re-count reversals | -20-30% reversals |

**Unconstrained Skills (Harder to Measure, Slower to Show Impact, but More Meaningful):**

| Skill | Measurement Approach | Expected Outcome |
|-------|---------------------|------------------|
| Creative Confidence | "I am good at drawing" (1-5 Likert scale) | +0.5-1.0 point increase |
| Handwriting Self-Efficacy | "I can write letters neatly" (1-5 Likert scale) | +0.5-1.0 point increase |
| Intrinsic Motivation | "I like practicing letters" (1-5 Likert scale) | +0.5-1.0 point increase |
| Spatial Reasoning | TVPS (Test of Visual Perceptual Skills) subscale | May not change in 4 weeks (longer study needed) |

**Study Design Example (for post-launch validation):**
- Recruit 30 kids who fit target profile
- Randomize to Scribbly (n=20) vs. control/wait-list (n=10)
- Pre-test all kids on letter formation (5 target letters: A, B, D, P, S)
- Scribbly group uses app 15 min/day, 3x/week for 4 weeks
- Control group uses traditional workbook 15 min/day, 3x/week
- Post-test letter formation with blinded scorer (doesn't know which group)
- Compare improvement: Did Scribbly group improve more?

**Gold Standard:** Publish results in peer-reviewed journal (Reading Research Quarterly, Journal of Educational Psychology) for maximum credibility.

### 6.4 Analytics Metrics (Post-Launch App Tracking)

Once app is live, track these engagement metrics:

| Metric | Definition | Target |
|--------|------------|--------|
| Session Length | Average time per app open | 10-12 minutes |
| Sessions per Week | Frequency of use | 3-5x/week |
| Letter Completion Rate | % of started letters that get traced AND turned into art | â‰¥75% |
| Creative Tool Usage | Which tools do kids use most? (stamps, colors, etc.) | All tools used by â‰¥30% of users |
| Haptic Settings | What do most kids set haptic to? (Off/Low/High) | Expect 30% Off, 50% Low, 20% High |
| Return Rate | % of users who open app after first session | â‰¥60% day 2, â‰¥40% week 2 |
| Churn Rate | % who stop using after trial/first month | â‰¤30% in first month |

**Use cases for analytics:**
- If creative tool X never used â†’ consider removing
- If haptic always set to "Off" â†’ question value of feature
- If session length <5 minutes â†’ content may be too short or boring
- If return rate <40% â†’ onboarding or first impression needs work

---

## 7. Scribbly MVP Testing Protocol (Step-by-Step)

### Phase 1: Internal Validation (Week 1-2)

**Goal:** Validate core concept before investing in prototype
**Participants:** 3-4 parent colleagues with 6-year-olds
**Materials:** Paper sketches or simple clickable wireframes

**Process:**
1. Show parent the concept: "App teaches letter tracing, then kids turn letters into art"
2. Walk through paper flow: trace A â†’ fade â†’ draw art using A shape
3. Ask parent:
   - "Would this appeal to your child?"
   - "Do you see educational value?"
   - "What concerns do you have?"
4. If possible, have their child try paper version (facilitator acts as "computer")

**Success Criteria:**
- â‰¥3/4 parents see clear educational value
- â‰¥3/4 parents say their child would engage
- No major concept-level concerns

**Decision:** Proceed to prototype if success criteria met.

---

### Phase 2: Medium-Fidelity Prototype Testing (Week 3-5)

**Goal:** Uncover major usability issues, validate trace-to-create flow
**Participants:** 5-7 child-parent pairs (age 6)
**Materials:** Interactive Figma/Adobe XD prototype with 2-3 letters

**Recruitment:**
- Post on local parent Facebook groups
- Reach out to elementary schools
- Offer $50 Amazon gift card for 30-minute session

**Session Structure (30 minutes total):**

**1. Welcome & Consent (3 min)**
- Greet parent and child
- Confirm parent signed consent form
- Get child assent: "Want to try a new drawing app and tell me what you think?"

**2. Child Testing (12 min)**
- Hand child tablet with prototype
- Say: "This is an app where you trace letters and then make art. Can you show me how you would use it?"
- Observe silently, take notes on:
  - Where child looks first
  - Any confusion points
  - Engagement level (smiling, focused, frustrated)
  - When they need help
- After 10 minutes or 2 letters, whichever comes first, ask:
  - "What did you think?"
  - "Would you want to use this again?"
  - "Was anything hard?"

**3. Parent Interview (12 min)**
- Child plays quietly nearby or leaves with other parent
- Ask parent:
  - "What was your impression watching [child] use it?"
  - "Do you think it would help with handwriting? Why?"
  - "What concerns do you have?"
  - "What would you want to know before downloading?"
  - "What price seems fair for an app like this?"
- Show pricing options, get reaction

**4. Wrap-Up (3 min)**
- Thank them, give gift card and child sticker
- Ask if they'd be willing to test again in 4-6 weeks (note for future)

**Data Collection:**
- Take detailed observation notes during child session
- Record parent interview (with permission) or take detailed notes
- After each session, rate child on:
  - Engagement (1-5)
  - Task completion (%)
  - Frustration level (1-5)
  - Delight moments (#)

**Analysis (After all 5-7 sessions):**
- Identify patterns: Where did 3+ kids struggle?
- Affinity map parent concerns
- Calculate: % who'd use again, average purchase intent score
- List top 5 usability issues to fix
- Estimate price sensitivity (median acceptable price)

**Success Criteria:**
- â‰¥80% of kids complete 1 full letter (trace + creative)
- â‰¥70% say they'd use again
- â‰¥60% of parents see educational value (â‰¥7/10 rating)
- â‰¥50% purchase intent (Yes or Maybe)

**Decision:** If success criteria not met, iterate prototype and test with 3-4 new families. If met, proceed to hi-fi.

---

### Phase 3: High-Fidelity Prototype Testing (Week 6-8)

**Goal:** Final validation before development, measure purchase intent
**Participants:** 8-10 child-parent pairs (can include 2-3 from Phase 2 to show progress)
**Materials:** Polished prototype with 5-7 letters, realistic visuals, animations

**Process:** Same as Phase 2, but with these additions:
- Test for longer (15 minutes child session if willing)
- Ask parent: "If this were in App Store today for $7.99/month, would you buy it? Why or why not?"
- Show competitive apps (LetterSchool, ABC Kids), ask: "How does Scribbly compare?"

**Success Criteria:**
- â‰¥85% task completion
- â‰¥80% would use again
- â‰¥70% parents see strong educational value (â‰¥8/10)
- â‰¥60% purchase intent at target price point
- â‰¥50% prefer Scribbly over competitor shown

**Decision:** Proceed to development if criteria met. If not, iterate and test with 3-5 new families.

---

### Phase 4: Technical Demo Testing (Week 6-8, Parallel to Phase 3)

**Goal:** Validate haptic feedback and drawing engine before full build
**Participants:** 3-5 kids age 6
**Materials:** Simple working app with 1-2 letters, real haptics, real drawing engine

**Process:**
- 10-minute session, no parent interview needed
- Focus ONLY on technical features:
  - Can they feel haptic feedback differences?
  - Does it help or distract?
  - Is tracing tolerance right? (Â¼ inch)
  - Does drawing engine feel responsive?

**Success Criteria:**
- â‰¥60% kids notice haptic feedback difference (Off vs Low vs High)
- â‰¥50% say haptic "helps" when asked
- â‰¤20% get frustrated by tracing tolerance
- No major lag or technical issues

**Decision:** Confirms technical approach before full development investment.

---

## 8. Post-Launch Validation Study (Months 3-6)

Once app is live and being used, conduct a small learning effectiveness study for marketing credibility.

**Study Design:**
- Recruit 30-40 kids through app (offer free premium for participation)
- Pre-test: Kids write 5 target letters (A, B, D, P, S), score legibility 1-5
- Intervention: Use Scribbly 15 min/day, 3x/week for 4 weeks
- Post-test: Re-write same 5 letters, score by blinded evaluator
- Compare: Did letter quality improve? (Expect +0.5 to +1.0 points/letter)

**Optional Control Group:**
- If resources allow, recruit 20 kids NOT using Scribbly as comparison

**Outcome:**
- Publish results on website: "Children improved letter formation by 25% after 4 weeks"
- Submit to education conference or journal
- Use in marketing materials and investor pitch

**Investment:** $5,000-10,000 (researcher time, recruitment incentives, data analysis)

---

## 9. Budget Estimate for Full MVP Testing

| Phase | Activities | Cost Estimate |
|-------|-----------|---------------|
| **Phase 1: Internal** | Informal tests with parent colleagues | $0-500 (gift cards) |
| **Phase 2: Med-Fi Prototype** | 5-7 child-parent pairs @ $50 each | $250-350 |
| **Phase 3: Hi-Fi Prototype** | 8-10 child-parent pairs @ $75 each | $600-750 |
| **Phase 4: Technical Demo** | 3-5 kids @ $50 each (shorter session) | $150-250 |
| **Recruitment** | Facebook ads, school outreach, platforms | $500-1,000 |
| **Prototyping Tools** | Figma Pro, recording software, devices | $200-500 |
| **Analysis** | UX researcher time (if external) | $2,000-5,000 |
| **TOTAL MVP TESTING** | | **$3,700-8,350** |

*Note: If testing in-house (founder or team member as facilitator), can save $2-5k on external researcher costs.*

**Post-Launch Learning Study:** Add $5,000-10,000 for longitudinal effectiveness research.

---

## 10. Key Takeaways & Recommended Approach

### Critical Success Factors

1. **Prioritize child observation over verbal feedback**
   - 6-year-olds won't tell you what's wrong; their behavior will
   - Watch for frustration, confusion, delightâ€”these reveal UX truth

2. **Parents validate educational value**
   - Kids determine if it's usable and fun
   - Parents determine if it's worth paying for
   - Both must pass for product success

3. **Test iteratively in phases**
   - Don't build full app then test (too risky)
   - Prototype â†’ Test â†’ Fix â†’ Re-prototype â†’ Test â†’ Build
   - Each testing phase de-risks the next investment

4. **Balance sample size with resources**
   - 5-7 kids finds 80-85% of issues
   - 8-10 kids finds 90%+ and provides better parent data
   - More than 15 per round has diminishing returns

5. **Validate technical features early**
   - Haptic feedback can't be prototypedâ€”build demo app early
   - Don't wait until full development to find tech doesn't work

### Recommended MVP Testing Timeline

**Total Duration:** 8-10 weeks from concept to validated prototype

- **Weeks 1-2:** Internal validation (paper prototypes, parent colleagues)
- **Weeks 3-5:** Medium-fidelity prototype + first usability testing round (5-7 pairs)
- **Weeks 6-8:** High-fidelity prototype + final testing round (8-10 pairs)
- **Parallel Week 6-8:** Technical demo testing (haptics, drawing engine)
- **Week 9-10:** Analysis, final iteration, handoff to development

**Total Testing Investment:** $3,700-8,350
**Total Participants:** 16-24 child-parent pairs across all phases

### What Success Looks Like

**By end of MVP testing, you should have:**
âœ… Validated that 6-year-olds can independently use Scribbly
âœ… Confirmed trace-to-create flow is clear and engaging
âœ… Identified and fixed 90%+ of major usability issues
âœ… Evidence that parents see educational value (â‰¥7/10 rating)
âœ… Purchase intent data showing â‰¥60% would pay target price
âœ… Validated haptic feedback and drawing engine work properly
âœ… Confidence to invest in full development

**After launch, you'll add:**
âœ… Learning effectiveness study showing measurable improvement
âœ… Published results for credibility and marketing
âœ… Longitudinal data on engagement and retention

---

## 11. Resources & References

### Testing Tools & Platforms
- **Recruitment:** UserTesting.com, Respondent.io, Facebook Parent Groups, Local Schools
- **Prototyping:** Figma, Adobe XD, ProtoPie, InVision
- **Recording:** Lookback.io, Zoom (with permission), UserTesting platform
- **Analysis:** Miro (affinity mapping), Dovetail (research repository), Google Sheets

### Key Research Papers
- Nielsen, J. (2000). "Why You Only Need to Test with 5 Users" - Nielsen Norman Group
- Faulkner, L. (2003). "Beyond the five-user assumption" - Behavior & IT
- Radesky, J. et al. (2021). "How educational are 'educational' apps?" - Four Pillars Framework
- Kim, J. et al. (2021). "Measures Matter: Meta-Analysis of Educational Apps" - SAGE
- Hirsh-Pasek, K. et al. (2015). "Putting Education in Educational Apps" - Psychological Science

### Ethical Guidelines
- HHS.gov: "Research with Children FAQs"
- FDA: "Ethical Considerations for Clinical Investigations Involving Children"
- COPPA (Children's Online Privacy Protection Act)
- Nielsen Norman Group: "Usability Testing with Minors: 16 Tips"

### Child Development Resources
- National Association for the Education of Young Children (NAEYC)
- Common Sense Media: Educational App Reviews
- PBS KIDS: Design Principles for Children's Media
- Joan Ganz Cooney Center: Educational Technology Research

---

## Conclusion

User testing with children requires specialized protocols that balance research rigor with ethical protection. By following this framework, Scribbly can validate both **usability** (can 6-year-olds actually use it?) and **learning effectiveness** (does it actually teach?) before investing heavily in development.

**The MVP testing approachâ€”8-10 weeks, 16-24 child-parent pairs, $3,700-8,350â€”provides the evidence needed to confidently move forward with development while minimizing risk.**

After launch, a follow-up learning effectiveness study (30-40 kids, 4 weeks, $5-10k) will provide the quantitative data needed for marketing claims and long-term credibility.

**Most importantly:** This testing validates the core hypothesis that children can learn letter formation through play, without judgment, in a way that builds both skills and creative confidence. That's the promise of Scribblyâ€”now go test it.

---

**Next Steps:**
1. Review this framework with team
2. Allocate testing budget ($4-8k for MVP)
3. Recruit Phase 1 participants (3-4 parent colleagues)
4. Build lo-fi prototype (2 weeks)
5. Begin testing timeline (8-10 weeks to validated prototype)

Good luck! ðŸŽ¨âœï¸
